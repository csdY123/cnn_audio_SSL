# dataset_sim.py
import torch
from torch.utils.data import Dataset
import numpy as np
import pyroomacoustics as pra
import librosa
import scipy.signal as signal
import soundfile as sf
import os
import glob
import random

class DynamicRoomSimulator(Dataset):
    def __init__(self, audio_source_dir, sample_length=2048, epoch_length=2000,
                 noise_dir="/mnt/chensenda/codes/sound/denoisy/datasets/DNS-Challenge/datasets/noise"):
        self.sample_length = sample_length
        self.epoch_length = epoch_length
        self.fs = 16000
        
        # -------------------------------------------------------------
        # 1. Load speech source files (supports .wav and .flac)
        # -------------------------------------------------------------
        self.source_files = []
        for ext in ['*.wav', '*.flac']:
            self.source_files.extend(glob.glob(os.path.join(audio_source_dir, f"**/{ext}"), recursive=True))
            self.source_files.extend(glob.glob(os.path.join("/mnt/chensenda/codes/sound/cnn_audio_SSL/speech_data/data_aishell/wav/train", f"**/{ext}"), recursive=True))
            
            
        if len(self.source_files) == 0:
            raise ValueError(f"No audio files found in {audio_source_dir} (.wav/.flac)!")
        
        print(f"[DynamicRoomSimulator] Loaded {len(self.source_files)} source audio files")

        # -------------------------------------------------------------
        # 2. Load real noise files into memory for fast training
        # Using soundfile instead of librosa for better performance
        # -------------------------------------------------------------
        self.noise_cache = []  # Pre-loaded noise arrays in memory
        if noise_dir and os.path.exists(noise_dir):
            noise_files = []
            for ext in ['*.wav', '*.flac']:
                noise_files.extend(glob.glob(os.path.join(noise_dir, f"**/{ext}"), recursive=True))
            
            print(f"[DynamicRoomSimulator] Found {len(noise_files)} noise files, pre-loading to memory...")
            
            # Pre-load all noise files into memory for fast access during training
            for nf in noise_files:
                try:
                    # Use soundfile for fast loading (much faster than librosa)
                    data, sr = sf.read(nf, dtype='float32')
                    
                    # Handle stereo -> mono conversion
                    if len(data.shape) > 1:
                        data = np.mean(data, axis=1)
                    
                    # Resample to target sample rate if needed
                    if sr != self.fs:
                        # Use scipy for fast resampling
                        num_samples = int(len(data) * self.fs / sr)
                        data = signal.resample(data, num_samples)
                    
                    # Only keep noise clips longer than 0.5 seconds
                    if len(data) >= self.fs * 0.5:
                        self.noise_cache.append(data.astype(np.float32))
                except Exception as e:
                    # Skip problematic files silently
                    pass
            
            print(f"[DynamicRoomSimulator] Successfully cached {len(self.noise_cache)} noise clips in memory")
        else:
            print(f"[DynamicRoomSimulator] WARNING: noise_dir not found, using Gaussian noise as fallback")
        
        # Unitree Go2 microphone array definition (4 mics, 3D coordinates)
        self.mic_positions_local = np.array([
            [ 0.1035,  0.0235, 0.0], # mic0
            [ 0.1035, -0.0235, 0.0], # mic1
            [-0.1035, -0.0235, 0.0], # mic2
            [-0.1035,  0.0235, 0.0], # mic3
        ]).T # Transpose to 3x4

    def __len__(self):
        return self.epoch_length

    def _get_random_room_params(self):
        """
        æ”¹è¿›ç‰ˆï¼šéšæœºç”Ÿæˆä¸åŒç±»å‹çš„æˆ¿é—´ï¼ˆå°ã€ä¸­ã€å¤§ã€é•¿èµ°å»Šï¼‰
        è¦†ç›– 3m - 15m çš„èŒƒå›´ï¼Œé€‚åº”æœºå™¨ç‹—çš„çœŸå®æ´»åŠ¨åŒºåŸŸ
        """
        room_type = np.random.choice(['small', 'medium', 'large', 'corridor'], p=[0.3, 0.4, 0.2, 0.1])

        if room_type == 'small':
            # å§å®¤ã€å°ä¹¦æˆ¿ (3-5m)
            l = np.random.uniform(3.0, 5.0)
            w = np.random.uniform(3.0, 5.0)
            h = np.random.uniform(2.5, 3.0)
            rt60 = np.random.uniform(0.2, 0.4)
            
        elif room_type == 'medium':
            # å®¢å…ã€ä¼šè®®å®¤ (5-8m) - ä½ åŸæ¥çš„è®¾ç½®ä¸»è¦è¦†ç›–è¿™é‡Œ
            l = np.random.uniform(5.0, 8.0)
            w = np.random.uniform(5.0, 8.0)
            h = np.random.uniform(2.8, 3.5)
            rt60 = np.random.uniform(0.3, 0.6)
            
        elif room_type == 'large':
            # å¤§å…ã€å¼€æ”¾å¼åŠå…¬åŒº (8-15m)
            # ğŸ”¥ å…³é”®ï¼šåªæœ‰è¶³å¤Ÿå¤§çš„æˆ¿é—´æ‰èƒ½å®¹çº³ 5m+ çš„å£°æºè€Œä¸è´´å¢™
            l = np.random.uniform(8.0, 15.0)
            w = np.random.uniform(8.0, 15.0)
            h = np.random.uniform(3.5, 5.0) # å¤§æˆ¿é—´é€šå¸¸é¡¶æ›´é«˜
            rt60 = np.random.uniform(0.5, 0.9) # æ··å“æ›´é•¿
            
        elif room_type == 'corridor':
            # èµ°å»Š (ç‹­é•¿) - æœºå™¨ç‹—å¸¸è§åœºæ™¯
            # ç‰¹å¾ï¼šä¸€ä¸ªç»´åº¦å¾ˆé•¿ï¼Œå¦ä¸€ä¸ªç»´åº¦å¾ˆçª„
            if np.random.random() < 0.5:
                l = np.random.uniform(10.0, 20.0) # é•¿
                w = np.random.uniform(2.0, 3.5)   # çª„
            else:
                l = np.random.uniform(2.0, 3.5)
                w = np.random.uniform(10.0, 20.0)
            h = np.random.uniform(2.5, 3.5)
            rt60 = np.random.uniform(0.4, 0.7)

        room_dim = np.array([l, w, h])

        # ä½¿ç”¨ Sabine å…¬å¼åæ¨å¸éŸ³ç³»æ•°
        try:
            e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)
            # é™åˆ¶ max_orderï¼Œå¤§æˆ¿é—´å¦‚æœ order å¤ªé«˜è®¡ç®—ä¼šææ…¢
            # å¤§æˆ¿é—´ order=3 è¶³å¤Ÿæ¨¡æ‹Ÿé•¿æ‹–å°¾ï¼Œå°æˆ¿é—´å¯ä»¥ç”¨ order=5
            target_order = 3 if (l > 10 or w > 10) else 5 
            max_order = min(max_order, target_order) 
        except:
            e_absorption, max_order = 0.3, 3

        return room_dim, e_absorption, max_order

    def _random_pitch_shift(self, audio: np.ndarray, prob: float = 0.7) -> np.ndarray:
        """
        Random pitch shift via resampling to simulate different voice timbres.
        
        This is CRITICAL for preventing "timbre overfitting" when you have limited
        source audio files. By changing the playback speed:
          - Speed x 0.8 -> Simulates deep male voice / elderly
          - Speed x 1.2 -> Simulates high-pitched female voice / child
        
        Args:
            audio: Input audio signal (1D numpy array)
            prob: Probability of applying pitch shift (default 0.7)
        
        Returns:
            Pitch-shifted audio (same length as input)
        """
        if np.random.random() > prob:
            return audio
        
        # Random speed factor: 0.7x (very deep) to 1.4x (very high pitched)
        # This covers the range from bass male voices to children's voices
        speed_factor = np.random.uniform(0.7, 1.4)
        
        if abs(speed_factor - 1.0) < 0.05:
            # Skip if speed change is negligible
            return audio
        
        # Calculate new length after speed change
        original_len = len(audio)
        new_len = int(original_len / speed_factor)
        
        if new_len < 100:
            return audio
        
        # Resample to simulate speed change (pitch shift)
        # scipy.signal.resample is fast and maintains quality
        resampled = signal.resample(audio, new_len)
        
        # Adjust length back to original (crop or pad)
        if len(resampled) > original_len:
            # If slower (longer), crop from center
            start = (len(resampled) - original_len) // 2
            resampled = resampled[start:start + original_len]
        elif len(resampled) < original_len:
            # If faster (shorter), pad with zeros
            pad_total = original_len - len(resampled)
            pad_left = pad_total // 2
            pad_right = pad_total - pad_left
            resampled = np.pad(resampled, (pad_left, pad_right), mode='constant')
        
        return resampled.astype(np.float32)

    def _load_random_noise_clip(self, duration_samples: int) -> np.ndarray:
        """
        Load a random noise clip from the pre-cached noise buffer.
        This is used for point source noise simulation (e.g., TV, interfering speaker).
        
        Args:
            duration_samples: Desired length in samples
        
        Returns:
            Noise signal array of shape (duration_samples,)
        """
        if len(self.noise_cache) == 0:
            # Fallback: return Gaussian noise if no cached noise available
            return np.random.randn(duration_samples).astype(np.float32) * 0.1
        
        noise_y = random.choice(self.noise_cache).copy()
        
        # Adjust length to match duration
        if len(noise_y) < duration_samples:
            # Wrap-around padding for short noise clips
            noise_y = np.pad(noise_y, (0, duration_samples - len(noise_y)), mode='wrap')
        else:
            # Random crop for long noise clips
            max_start = len(noise_y) - duration_samples
            start_idx = np.random.randint(0, max_start + 1) if max_start > 0 else 0
            noise_y = noise_y[start_idx:start_idx + duration_samples]
        
        # Normalize to prevent clipping
        max_val = np.max(np.abs(noise_y))
        if max_val > 0:
            noise_y = noise_y / max_val
        
        return noise_y.astype(np.float32)

    def _load_random_source_clip(self, duration_sec=1.0):
        # Keep trying until we get a valid (non-empty) audio clip
        while True:
            wav_path = random.choice(self.source_files)
            # Get audio duration (librosa is slow, consider caching durations in production)
            full_duration = librosa.get_duration(path=wav_path)
            
            if full_duration <= duration_sec:
                offset = 0.0
            else:
                offset = np.random.uniform(0, full_duration - duration_sec)
            
            y, sr = librosa.load(wav_path, sr=self.fs, offset=offset, duration=duration_sec)
            
            # If empty, re-select another file
            if len(y) == 0:
                continue
            
            break
        
        # -----------------------------------------------------------
        # [Improvement 4] Pitch Shift to prevent timbre overfitting
        # Simulates different voice types: deep male -> high-pitched child
        # This effectively multiplies your 2703 files to 20000+ unique timbres
        # -----------------------------------------------------------
        y = self._random_pitch_shift(y, prob=0.4)
        
        # Normalize source signal to prevent clipping
        max_val = np.max(np.abs(y))
        if max_val > 0:
            y = y / max_val
        return y

    def __getitem__(self, idx):
        # -----------------------------------------------------------
        # 1. å‡†å¤‡ç¯å¢ƒ (éšæœºæˆ¿é—´å‚æ•°)
        # -----------------------------------------------------------
        room_dim, e_absorption, max_order = self._get_random_room_params()
        
        # åˆ›å»ºæˆ¿é—´
        room = pra.ShoeBox(
            room_dim, 
            fs=self.fs, 
            materials=pra.Material(e_absorption), 
            max_order=max_order
        )

        # -----------------------------------------------------------
        # 2. æ”¾ç½®éº¦å…‹é£ (éšæœºä½ç½®)
        # -----------------------------------------------------------
        # ä¿è¯é˜µåˆ—ç¦»å¢™è‡³å°‘ 0.5ç±³
        mic_center = np.array([
            np.random.uniform(0.5, room_dim[0] - 0.5),
            np.random.uniform(0.5, room_dim[1] - 0.5),
            np.random.uniform(0.5, room_dim[2] - 0.5) 
        ])
        
        current_mic_locs = self.mic_positions_local + mic_center.reshape(3, 1)
        room.add_microphone_array(current_mic_locs)

        # -----------------------------------------------------------
        # 3. æ”¾ç½®å£°æº (éšæœºè§’åº¦ + æ‰©å¤§è·ç¦» + é«˜åº¦æ‰°åŠ¨)
        # -----------------------------------------------------------
        angle_deg = np.random.randint(0, 360)
        angle_rad = np.deg2rad(angle_deg)
        
        # ğŸ”¥ã€æ”¹è¿›1ã€‘æ‰©å¤§è·ç¦»èŒƒå›´ï¼š0.5m (è¿‘åœº) åˆ° 5.0m (è¿œåœº)
        dist = np.random.uniform(0.5, 5.0)
        
        src_x = mic_center[0] + dist * np.cos(angle_rad)
        src_y = mic_center[1] + dist * np.sin(angle_rad)
        
        # ğŸ”¥ã€å®‰å…¨è¡¥ä¸ Aã€‘é«˜åº¦éšæœºæ‰°åŠ¨ (Z-axis Jitter)
        src_z = mic_center[2] + np.random.uniform(-0.2, 1.5)
        
        # -----------------------------------------------------------
        # [Fix] Ray Scaling instead of Hard Clip
        # Preserves angle (Direction), only scales distance
        # This prevents "Corner Magnetism" bias in angle distribution
        # -----------------------------------------------------------
        
        # 1. Define room boundaries (with 0.1m margin)
        x_min, x_max = 0.1, room_dim[0] - 0.1
        y_min, y_max = 0.1, room_dim[1] - 0.1
        z_min, z_max = 0.1, room_dim[2] - 0.1

        # 2. Calculate displacement vector from mic center
        dx = src_x - mic_center[0]
        dy = src_y - mic_center[1]
        dz = src_z - mic_center[2]

        # 3. Calculate scaling factor for each axis
        # If point is inside boundary, factor = 1.0; if outside, factor < 1.0
        scale = 1.0
        
        # X-axis boundary check
        if dx != 0:
            if src_x > x_max: 
                scale = min(scale, (x_max - mic_center[0]) / dx)
            if src_x < x_min: 
                scale = min(scale, (x_min - mic_center[0]) / dx)
        
        # Y-axis boundary check
        if dy != 0:
            if src_y > y_max: 
                scale = min(scale, (y_max - mic_center[1]) / dy)
            if src_y < y_min: 
                scale = min(scale, (y_min - mic_center[1]) / dy)
        
        # Z-axis boundary check (doesn't affect horizontal angle, but for physical correctness)
        if dz != 0:
            if src_z > z_max: 
                scale = min(scale, (z_max - mic_center[2]) / dz)
            if src_z < z_min: 
                scale = min(scale, (z_min - mic_center[2]) / dz)

        # Ensure scale is positive and reasonable
        # Only use min(scale, 1.0) to strictly respect wall boundaries
        # Use 1e-4 as floor to prevent negative scale from floating point errors
        scale = max(1e-4, min(scale, 1.0))

        # 4. Apply scaling to get final coordinates
        # Source is now guaranteed inside room, angle is preserved
        src_x = mic_center[0] + dx * scale
        src_y = mic_center[1] + dy * scale
        src_z = mic_center[2] + dz * scale

        # 5. Calculate final angle (should be same as original angle_deg, 
        #    but recalculate for floating point safety)
        real_dx = src_x - mic_center[0]
        real_dy = src_y - mic_center[1]
        real_angle_rad = np.arctan2(real_dy, real_dx)
        if real_angle_rad < 0: 
            real_angle_rad += 2 * np.pi
        
        # Final Label (0-359), add %360 to handle edge case
        label_deg = int(np.degrees(real_angle_rad)) % 360
        
        # Read random speech clip (0.5 seconds)
        source_signal = self._load_random_source_clip(duration_sec=0.5)
        
        # -----------------------------------------------------------
        # [Improvement 3] Spectral Augmentation via Random Bandpass Filter
        # Simulates high-frequency attenuation in real far-field recordings
        # and removes extreme low frequencies that may not be present
        # -----------------------------------------------------------
        if np.random.random() < 0.8:  # 80% probability to apply filter
            low_cut = np.random.uniform(50, 200)    # Random low cutoff: 50-200 Hz
            high_cut = np.random.uniform(4000, 7500)  # Random high cutoff: 3k-7k Hz
            try:
                sos = signal.butter(2, [low_cut, high_cut], btype='band', fs=self.fs, output='sos')
                source_signal = signal.sosfilt(sos, source_signal)
            except Exception:
                pass  # Skip filtering if parameters are invalid
        
        room.add_source([src_x, src_y, src_z], signal=source_signal)

        # -----------------------------------------------------------
        # [Improvement 5] Point Source Noise (Interfering Speaker / TV)
        # With 30% probability, add a directional noise source
        # This simulates realistic scenarios like another person talking nearby
        # The noise will have proper TDOA (phase differences) across microphones
        # -----------------------------------------------------------
        if np.random.random() < 0.3 and len(self.noise_cache) > 0:
            # Random angle for interference source (different from target)
            # Ensure at least 30 degrees separation from target source
            noise_angle_deg = (angle_deg + np.random.randint(30, 330)) % 360
            noise_angle_rad = np.deg2rad(noise_angle_deg)
            
            # Random distance for interference (typically closer for indoor scenarios)
            noise_dist = np.random.uniform(0.5, 3.0)
            
            # Calculate interference source position
            noise_x = mic_center[0] + noise_dist * np.cos(noise_angle_rad)
            noise_y = mic_center[1] + noise_dist * np.sin(noise_angle_rad)
            noise_z = mic_center[2] + np.random.uniform(-0.2, 1.0)
            
            # Apply same boundary scaling as target source
            ndx = noise_x - mic_center[0]
            ndy = noise_y - mic_center[1]
            ndz = noise_z - mic_center[2]
            
            nscale = 1.0
            if ndx != 0:
                if noise_x > x_max: nscale = min(nscale, (x_max - mic_center[0]) / ndx)
                if noise_x < x_min: nscale = min(nscale, (x_min - mic_center[0]) / ndx)
            if ndy != 0:
                if noise_y > y_max: nscale = min(nscale, (y_max - mic_center[1]) / ndy)
                if noise_y < y_min: nscale = min(nscale, (y_min - mic_center[1]) / ndy)
            if ndz != 0:
                if noise_z > z_max: nscale = min(nscale, (z_max - mic_center[2]) / ndz)
                if noise_z < z_min: nscale = min(nscale, (z_min - mic_center[2]) / ndz)
            nscale = max(1e-4, min(nscale, 1.0))
            
            noise_x = mic_center[0] + ndx * nscale
            noise_y = mic_center[1] + ndy * nscale
            noise_z = mic_center[2] + ndz * nscale
            
            # Load noise signal with same length as source signal
            noise_signal = self._load_random_noise_clip(len(source_signal))
            
            # Random gain for interference: 0.3x to 1.0x of target signal level
            # (interference should typically be weaker than target)
            interference_gain = np.random.uniform(0.0, 0.5)
            noise_signal = noise_signal * interference_gain
            
            room.add_source([noise_x, noise_y, noise_z], signal=noise_signal)

        # -----------------------------------------------------------
        # 4. è¿è¡Œç‰©ç†ä»¿çœŸ
        # -----------------------------------------------------------
        room.simulate()
        simulated_audio = room.mic_array.signals # Shape: (4, N)

        # -----------------------------------------------------------
        # [Improvement 1] Real Noise Injection instead of Gaussian noise
        # Uses pre-cached noise for fast training (no I/O in __getitem__)
        # -----------------------------------------------------------
        # Random SNR: 0dB (very noisy) - 25dB (quiet), expanded range for robustness
        target_snr_db = np.random.uniform(5.0, 25.0)
        
        sig_power = np.mean(simulated_audio ** 2)
        if sig_power > 0:
            if len(self.noise_cache) > 0:
                # Use pre-cached real noise (no I/O overhead!)
                noise_y = random.choice(self.noise_cache).copy()
                
                # Pad or truncate noise to match signal length
                signal_len = simulated_audio.shape[1]
                if len(noise_y) < signal_len:
                    # Wrap-around padding for short noise clips
                    noise_y = np.pad(noise_y, (0, signal_len - len(noise_y)), mode='wrap')
                else:
                    # Random crop for long noise clips
                    max_start = len(noise_y) - signal_len
                    start_idx = np.random.randint(0, max_start + 1) if max_start > 0 else 0
                    noise_y = noise_y[start_idx:start_idx + signal_len]
                
                # Expand single-channel noise to multi-channel (simulate diffuse field)
                # Use np.roll for circular shift to avoid silent gaps at the beginning
                noise_multichannel = np.zeros_like(simulated_audio)
                for ch in range(4):
                    # Random delay: 0-20 samples for physically realistic diffuse field
                    # Physical basis: mic spacing ~20cm -> max delay ~10 samples @16kHz
                    # Using 0-20 samples allows some extra decorrelation margin
                    # (Previous 0-100 samples was physically unrealistic)
                    delay = np.random.randint(0, 20)
                    # Use np.roll for circular shift (no data loss, no silent gaps)
                    noise_multichannel[ch, :] = np.roll(noise_y, shift=delay)
                    
                    # Also add small random gain per channel for diffuse field
                    noise_multichannel[ch] *= np.random.uniform(0.8, 1.2)
                
                noise_power = np.mean(noise_multichannel ** 2)
                if noise_power > 0:
                    scale = np.sqrt(sig_power / (noise_power * 10**(target_snr_db/10)))
                    simulated_audio = simulated_audio + noise_multichannel * scale
            else:
                # Fallback: Gaussian white noise (less realistic but better than nothing)
                noise_power = sig_power / (10 ** (target_snr_db / 10))
                noise = np.random.normal(0, np.sqrt(noise_power), simulated_audio.shape)
                simulated_audio = simulated_audio + noise
        
        # -----------------------------------------------------------
        # [Improvement 2] Channel Gain Perturbation
        # Simulates real-world microphone sensitivity mismatch (Â±1-3 dB)
        # This prevents the model from over-relying on amplitude differences
        # -----------------------------------------------------------
        gain_perturb = np.random.uniform(0.7, 1.3, size=(4, 1))  # ~Â±3dB
        simulated_audio = simulated_audio * gain_perturb

        # -----------------------------------------------------------
        # 5. åå¤„ç†ä¸éšæœºè£å‰ª (å«é™éŸ³é˜²å¾¡)
        # -----------------------------------------------------------
        # å½’ä¸€åŒ– (ä¿ç•™ ILD, é¿å…é™¤é›¶)
        max_amp = np.max(np.abs(simulated_audio))
        if max_amp > 0:
            simulated_audio = simulated_audio / max_amp * 0.9
        
        signal_len = simulated_audio.shape[1]
        
        if signal_len > self.sample_length:
            max_start = signal_len - self.sample_length
            
            # ğŸ”¥ã€å®‰å…¨è¡¥ä¸ Bã€‘é™éŸ³åˆ‡ç‰‡é˜²å¾¡ (Silent Crop Protection)
            # å°è¯• 3 æ¬¡éšæœºåˆ‡ç‰‡ï¼Œç¡®ä¿åˆ‡åˆ°çš„ç‰‡æ®µæœ‰è¶³å¤Ÿçš„èƒ½é‡
            for _ in range(3):
                start = np.random.randint(0, max_start + 1)
                cropped = simulated_audio[:, start : start + self.sample_length]
                
                # èƒ½é‡é˜ˆå€¼æ£€æµ‹ (1e-5 æ˜¯ç»éªŒå€¼)
                if np.mean(cropped**2) > 1e-5:
                    break
            else:
                # å¦‚æœ3æ¬¡éƒ½å¤±è´¥(æç½•è§)ï¼Œå…œåº•æ–¹æ¡ˆï¼šå–æ­£ä¸­é—´
                start = max_start // 2
                cropped = simulated_audio[:, start : start + self.sample_length]
                
        else:
            # è¡¥é›¶ (Padding)
            pad_width = self.sample_length - signal_len
            cropped = np.pad(simulated_audio, ((0,0), (0, pad_width)))

        # è½¬ Tensor
        audio_tensor = torch.from_numpy(cropped.astype(np.float32))
        
        # è¿”å›è§’åº¦ Label
        label_tensor = torch.tensor(label_deg, dtype=torch.long) 

        return audio_tensor, label_tensor